{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import csv\n",
    "import random\n",
    "import re\n",
    "from pathlib import Path\n",
    "from xml.etree.ElementTree import ElementTree\n",
    "import nltk \n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/abhishek/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn \n",
    "POS = {'NOUN':wn.NOUN, 'VERB':wn.VERB, 'ADJ':wn.ADJ, 'ADV':wn.ADV}\n",
    "\n",
    "def getInfo(type,pos,lemma):\n",
    "    res = dict()\n",
    "    word_pos = POS[pos] if pos is not None else None\n",
    "    morpho = wn._morphy(lemma, pos=word_pos) if pos is not None else []\n",
    "\n",
    "    for synset in tqdm(set(wn.synsets(lemma,pos=word_pos))):\n",
    "        key = None\n",
    "        for lem in synset.lemmas():\n",
    "            if lem.name().lower() == lemma.lower():\n",
    "                key = lem.key()\n",
    "                break\n",
    "            elif lem.name().lower() in morpho:\n",
    "                key = lem.key()\n",
    "            \n",
    "        assert key is not None\n",
    "        res[key] = synset.definition() if type == 'def' else synset.examples()\n",
    "    \n",
    "    return res\n",
    "\n",
    "def get_glosses(pos,lemma):\n",
    "    return getInfo('def',pos,lemma) \n",
    "\n",
    "def getexample(pos,lemma):\n",
    "    return getInfo('ex',pos,lemma)\n",
    "\n",
    "def getAllWordnetLemmaNames():\n",
    "    res = []\n",
    "    for pos, pos_name in POS.items():\n",
    "        for synset in wn.synsets(pos=pos_name):\n",
    "            res.append((pos,wn.all_lemma_names(pos=pos_name)))\n",
    "\n",
    "    return res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_file = './SemCor/semcor.data.xml'\n",
    "gold_txt_file = './SemCor/semcor.gold.key.txt'\n",
    "output_file = './SemCor/semcor_data.csv'\n",
    "max_glossKey = 4\n",
    "\n",
    "print(\"Creating dataset...\")\n",
    "root = ElementTree(file=xml_file).getroot()\n",
    "with open(output_file,'w',encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['id','sentence','sense_keys','glosses','target_words'])\n",
    "\n",
    "    def write_to_csv(id_,sentence_,lemma_,pos_,gold_keys_):\n",
    "        sense_i = get_glosses(pos_,lemma_)\n",
    "        # print(sense_i)\n",
    "        gloss_sense_pairs = list()\n",
    "        for i in gold_keys_:\n",
    "            gloss_sense_pairs.append((i,sense_i[i]))\n",
    "            del sense_i[i]\n",
    "        rem = max_glossKey - len(gloss_sense_pairs)\n",
    "        if len(sense_i) > rem :\n",
    "            gloss_sense_pairs.extend(random.sample(list(sense_i.items()),rem))\n",
    "        elif len(sense_i) > 0:\n",
    "            gloss_sense_pairs.extend(list(sense_i.items()))\n",
    "\n",
    "        random.shuffle(gloss_sense_pairs)\n",
    "        glosses = [i[1] for i in gloss_sense_pairs]\n",
    "        sense_keys = [i[0] for i in gloss_sense_pairs]\n",
    "\n",
    "        target_words = [sense_keys.index(i) for i in gold_keys_]\n",
    "        writer.writerow([id_,sentence_,sense_keys,glosses,target_words])\n",
    "\n",
    "    with open(gold_txt_file,'r',encoding='utf-8') as g:\n",
    "        for dc in tqdm(root):\n",
    "            for sentence in dc:\n",
    "                instances = list()\n",
    "                tokens = list()\n",
    "                for token in sentence:\n",
    "                    tokens.append(token.text)\n",
    "                    if token.tag == 'instance':\n",
    "                        strt_index = len(tokens) -1 \n",
    "                        end_index = strt_index + 1\n",
    "                        instances.append((token.attrib['id'],strt_index,end_index,token.attrib['lemma'],token.attrib['pos']))\n",
    "                # print(instances)\n",
    "                \n",
    "                for id_,start,end,lemma,pos in instances:\n",
    "                    gold_key = g.readline().strip().split()\n",
    "                    gold = gold_key[1:]\n",
    "                    assert id_ == gold_key[0]\n",
    "                    sentence_ = ' '.join(\n",
    "                        tokens[:start] + ['[TGT]'] + tokens[start:end] + ['[TGT]'] + tokens[end:]\n",
    "                    )\n",
    "                    write_to_csv(id_,sentence_,lemma,pos,gold)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read semeval2015.gold.txt \n",
    "with open(\"./senseval3.gold.key.txt\") as f:\n",
    "    lines = f.readlines()\n",
    "    lines = [line.strip() for line in lines]\n",
    "\n",
    "# read semeval2015.test.txt\n",
    "with open(\"./senseval3_predictions.txt\") as f:\n",
    "    test_lines = f.readlines()\n",
    "    test_lines = [line.strip() for line in test_lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7005405405405405\n"
     ]
    }
   ],
   "source": [
    "\n",
    "count = 0\n",
    "for i in range(len(lines)):\n",
    "    # split the line to get the id and the gloss\n",
    "    line = lines[i].split()\n",
    "    # print(line)\n",
    "    test_line = test_lines[i].split()\n",
    "    for j in range(len(line[1:])):\n",
    "        if test_line[1] == line[j+1]:\n",
    "            count += 1\n",
    "            break\n",
    "\n",
    "print(\"Accuracy:\",count/len(lines))\n",
    "# print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7354198262787812\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(len(lines)):\n",
    "    # split the line to get the id and the gloss\n",
    "    line = lines[i].split()\n",
    "    # print(line)\n",
    "    test_line = test_lines[i].split()\n",
    "    for j in range(len(line[1:])):\n",
    "        if test_line[1] == line[j+1]:\n",
    "            count += 1\n",
    "            break\n",
    "\n",
    "print(\"Accuracy:\", count/len(lines))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BERT_for_WSD: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BERT_for_WSD from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BERT_for_WSD from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BERT_for_WSD were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['ranking_linear.bias', 'ranking_linear.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 14/14 [00:00<00:00, 92618.70it/s]\n",
      "Progress: 100%|██████████| 14/14 [00:01<00:00,  8.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions:\n",
      "  No.  Sense key     Definition                                                                             Score\n",
      "-----  ------------  -----------------------------------------------------------------------------------  -------\n",
      "    1  be%1:27:00::  a light strong brittle grey toxic bivalent metallic element                          0.08185\n",
      "    2  be%2:41:00::  work in a specific place, with a specific subject, or in a specific function         0.08056\n",
      "    3  be%2:42:05::  occupy a certain position or area; be somewhere                                      0.079\n",
      "    4  be%2:42:04::  happen, occur, take place                                                            0.07449\n",
      "    5  be%2:42:13::  to remain unmolested, undisturbed, or uninterrupted -- used only in infinitive form  0.07264\n",
      "    6  be%2:42:00::  have an existence, be extant                                                         0.07008\n",
      "    7  be%2:42:03::  have the quality of being; (copula, used with an adjective or a predicate noun)      0.06961\n",
      "    8  be%2:40:00::  spend or use time                                                                    0.06834\n",
      "    9  be%2:42:06::  be identical to; be someone or something                                             0.06796\n",
      "   10  be%2:42:08::  represent, as of a character on stage                                                0.06776\n",
      "   11  be%2:42:02::  form or compose                                                                      0.06755\n",
      "   12  be%2:42:01::  have life, be alive                                                                  0.0675\n",
      "   13  be%2:42:09::  be priced at                                                                         0.06636\n",
      "   14  be%2:42:07::  be identical or equivalent to                                                        0.0663\n",
      "\n",
      "Incorrect input format. Please try again.\n",
      "\n",
      "Incorrect input format. Please try again.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import re\n",
    "\n",
    "import torch\n",
    "from tabulate import tabulate\n",
    "from torch.nn.functional import softmax\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer\n",
    "from createFeatures import GlossSelectionRecord, _create_features_from_records\n",
    "from modelBERT import BERT_for_WSD\n",
    "\n",
    "\n",
    "MAX_SEQ_LENGTH = 128\n",
    "device  = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "def get_predictions(model, tokenizer, sentence):\n",
    "    re_result = re.search(r\"\\[TGT\\](.*)\\[TGT\\]\", sentence)\n",
    "    if re_result is None:\n",
    "        print(\"Incorrect input format\")\n",
    "        return\n",
    "\n",
    "    ambiguous_word = re_result.group(1).strip()\n",
    "    sense_keys = []\n",
    "    definitions = []\n",
    "    for sense_key, definition in get_glosses(None,ambiguous_word).items():\n",
    "        sense_keys.append(sense_key)\n",
    "        definitions.append(definition)\n",
    "\n",
    "    record = GlossSelectionRecord(\n",
    "        \"test\", sentence, sense_keys, definitions, [-1])\n",
    "    features = _create_features_from_records([record], MAX_SEQ_LENGTH, tokenizer,\n",
    "                                             cls_token=tokenizer.cls_token,\n",
    "                                             sep_token=tokenizer.sep_token,\n",
    "                                             cls_token_segment_id=1,\n",
    "                                             pad_token_segment_id=0,\n",
    "                                             disable_progress_bar=True)[0]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = torch.zeros(len(definitions), dtype=torch.double).to(device)\n",
    "        for i, bert_input in tqdm(list(enumerate(features)), desc=\"Progress\"):\n",
    "            logits[i] = model.ranking_linear(\n",
    "                model.bert(\n",
    "                    input_ids=torch.tensor(\n",
    "                        bert_input.input_ids, dtype=torch.long).unsqueeze(0).to(device),\n",
    "                    attention_mask=torch.tensor(\n",
    "                        bert_input.input_mask, dtype=torch.long).unsqueeze(0).to(device),\n",
    "                    token_type_ids=torch.tensor(\n",
    "                        bert_input.segment_ids, dtype=torch.long).unsqueeze(0).to(device)\n",
    "                )[1]\n",
    "            )\n",
    "        scores = softmax(logits, dim=0)\n",
    "\n",
    "    return sorted(zip(sense_keys, definitions, scores), key=lambda x: x[-1], reverse=True)\n",
    "\n",
    "\n",
    "model = BERT_for_WSD.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "while True:\n",
    "    sentence = input(\n",
    "        \"\\nEnter a sentence with an ambiguous word surrounded by [TGT] tokens\\n> \")\n",
    "    predictions = get_predictions(model, tokenizer, sentence)\n",
    "    if predictions:\n",
    "        print(\"\\nPredictions:\")\n",
    "        print(tabulate(\n",
    "            [[f\"{i+1}.\", key, gloss, f\"{score:.5f}\"]\n",
    "             for i, (key, gloss, score) in enumerate(predictions)],\n",
    "            headers=[\"No.\", \"Sense key\", \"Definition\", \"Score\"])\n",
    "        )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
