{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/abhishek/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from nltk.corpus import wordnet as wn\n",
    "# puncutaiton \n",
    "import string\n",
    "\n",
    "\n",
    "data = pd.read_csv('../NN/SemCor/semcor_copy.csv')\n",
    "# data2 = pd.read_csv('semcor4.csv')\n",
    "# data = pd.concat([data1, data2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_lesk(cotext_sentence,amb_word):\n",
    "    max_overlap = 0\n",
    "    lesk_sense = None\n",
    "    context_words = nltk.word_tokenize(cotext_sentence)\n",
    "    context_words = set(context_words)\n",
    "    # print(wn.synsets(amb_word))\n",
    "    for sense in wn.synsets(amb_word):\n",
    "        print(sense)\n",
    "        signature = set()\n",
    "        sene_definitions = nltk.word_tokenize(sense.definition())\n",
    "        signature = signature.union(set(sene_definitions))\n",
    "        signature = signature.union(set(sense.lemma_names()))\n",
    "        for example in sense.examples():\n",
    "            signature = signature.union(set(example.split()))\n",
    "        overlap = len(context_words.intersection(signature))\n",
    "        if overlap > max_overlap:\n",
    "            lesk_sense = sense\n",
    "            max_overlap = overlap\n",
    "\n",
    "    return lesk_sense\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversion(sense):\n",
    "    trg = None \n",
    "    long_lemma = wn.lemma_from_key(sense)\n",
    "    long_synset = long_lemma.synset()\n",
    "    trg = long_synset.name()\n",
    "    return trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(algorithm, data):\n",
    "    correct = 0\n",
    "    for i in range(len(data)):\n",
    "        context = data['context'][i]\n",
    "        word = data['sense_full'][i]\n",
    "        word = word.split('.')[0]\n",
    "        pred_sense = algorithm(context, word)\n",
    "        print(pred_sense)\n",
    "        if pred_sense is None:\n",
    "            continue\n",
    "\n",
    "        if data['sense_full'][i] == pred_sense.name():\n",
    "            correct += 1\n",
    "\n",
    "    return correct/len(data)*100\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy of Simple Lesk Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy(simple_lesk, data), \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pprint\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_en = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def tokenize(document, word):\n",
    "    tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(document)\n",
    "    tokens = [\n",
    "        token for token in tokens if token not in stopwords_en and token.isalpha()]\n",
    "    tokens = [token for token in tokens if token != word]\n",
    "    return set(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extended_lesk(context, word):\n",
    "    context = context.lower()\n",
    "    word = word.lower()\n",
    "\n",
    "    context_tokens = tokenize(context, word)\n",
    "\n",
    "    # calculating the word sense disambiguation using simple LESK\n",
    "    synsets = wordnet.synsets(word)\n",
    "    # print(synsets)\n",
    "    finWeights = [0] * len(synsets)\n",
    "    N_t = len(synsets)\n",
    "    weights= {}\n",
    "\n",
    "    for context_token in context_tokens :\n",
    "        weights[context_token] = 1\n",
    "        for sense in synsets:\n",
    "            if context_token in sense.definition():\n",
    "                weights[context_token] += N_t\n",
    "                continue\n",
    "\n",
    "            for example in sense.examples():\n",
    "                if context_token in example:\n",
    "                    weights[context_token] += N_t\n",
    "                    break\n",
    "\n",
    "            for lemma in sense.lemma_names():\n",
    "                if context_token in lemma:\n",
    "                    weights[context_token] += N_t\n",
    "                    break\n",
    "\n",
    "    for ind,sense in enumerate(synsets):\n",
    "        overlap = set()\n",
    "        for example in sense.examples():\n",
    "            for token in tokenize(example, word):\n",
    "                overlap.add(token)\n",
    "\n",
    "        for token in tokenize(sense.definition(), word):\n",
    "            overlap.add(token)\n",
    "\n",
    "        for token in sense.lemma_names():\n",
    "            overlap.add(token)\n",
    "\n",
    "        for token in context_tokens:\n",
    "            if token in overlap:\n",
    "                finWeights[ind] += np.log(weights[token] / N_t)\n",
    "    if len(finWeights) == 0:\n",
    "        return None\n",
    "    max_weight = max(finWeights)\n",
    "    index = finWeights.index(max_weight)\n",
    "    return synsets[index]\n",
    "                \n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy of Extended Lesk Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.71681020020132 %\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(extended_lesk, data), \"%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
