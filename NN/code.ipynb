{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import csv\n",
    "import random\n",
    "import re\n",
    "from pathlib import Path\n",
    "from xml.etree.ElementTree import ElementTree\n",
    "import nltk \n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/abhishek/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn \n",
    "POS = {'NOUN':wn.NOUN, 'VERB':wn.VERB, 'ADJ':wn.ADJ, 'ADV':wn.ADV}\n",
    "\n",
    "def getInfo(type,pos,lemma):\n",
    "    res = dict()\n",
    "    word_pos = POS[pos] if pos is not None else None\n",
    "    morpho = wn._morphy(lemma, pos=word_pos) if pos is not None else []\n",
    "\n",
    "    for synset in tqdm(set(wn.synsets(lemma,pos=word_pos))):\n",
    "        key = None\n",
    "        for lem in synset.lemmas():\n",
    "            if lem.name().lower() == lemma.lower():\n",
    "                key = lem.key()\n",
    "                break\n",
    "            elif lem.name().lower() in morpho:\n",
    "                key = lem.key()\n",
    "            \n",
    "        assert key is not None\n",
    "        res[key] = synset.definition() if type == 'def' else synset.examples()\n",
    "    \n",
    "    return res\n",
    "\n",
    "def get_glosses(pos,lemma):\n",
    "    return getInfo('def',pos,lemma) \n",
    "\n",
    "def getexample(pos,lemma):\n",
    "    return getInfo('ex',pos,lemma)\n",
    "\n",
    "def getAllWordnetLemmaNames():\n",
    "    res = []\n",
    "    for pos, pos_name in POS.items():\n",
    "        for synset in wn.synsets(pos=pos_name):\n",
    "            res.append((pos,wn.all_lemma_names(pos=pos_name)))\n",
    "\n",
    "    return res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_file = './SemCor/semcor.data.xml'\n",
    "gold_txt_file = './SemCor/semcor.gold.key.txt'\n",
    "output_file = './SemCor/semcor_data.csv'\n",
    "max_glossKey = 4\n",
    "\n",
    "print(\"Creating dataset...\")\n",
    "root = ElementTree(file=xml_file).getroot()\n",
    "with open(output_file,'w',encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['id','sentence','sense_keys','glosses','target_words'])\n",
    "\n",
    "    def write_to_csv(id_,sentence_,lemma_,pos_,gold_keys_):\n",
    "        sense_i = get_glosses(pos_,lemma_)\n",
    "        # print(sense_i)\n",
    "        gloss_sense_pairs = list()\n",
    "        for i in gold_keys_:\n",
    "            gloss_sense_pairs.append((i,sense_i[i]))\n",
    "            del sense_i[i]\n",
    "        rem = max_glossKey - len(gloss_sense_pairs)\n",
    "        if len(sense_i) > rem :\n",
    "            gloss_sense_pairs.extend(random.sample(list(sense_i.items()),rem))\n",
    "        elif len(sense_i) > 0:\n",
    "            gloss_sense_pairs.extend(list(sense_i.items()))\n",
    "\n",
    "        random.shuffle(gloss_sense_pairs)\n",
    "        glosses = [i[1] for i in gloss_sense_pairs]\n",
    "        sense_keys = [i[0] for i in gloss_sense_pairs]\n",
    "\n",
    "        target_words = [sense_keys.index(i) for i in gold_keys_]\n",
    "        writer.writerow([id_,sentence_,sense_keys,glosses,target_words])\n",
    "\n",
    "    with open(gold_txt_file,'r',encoding='utf-8') as g:\n",
    "        for dc in tqdm(root):\n",
    "            for sentence in dc:\n",
    "                instances = list()\n",
    "                tokens = list()\n",
    "                for token in sentence:\n",
    "                    tokens.append(token.text)\n",
    "                    if token.tag == 'instance':\n",
    "                        strt_index = len(tokens) -1 \n",
    "                        end_index = strt_index + 1\n",
    "                        instances.append((token.attrib['id'],strt_index,end_index,token.attrib['lemma'],token.attrib['pos']))\n",
    "                # print(instances)\n",
    "                \n",
    "                for id_,start,end,lemma,pos in instances:\n",
    "                    gold_key = g.readline().strip().split()\n",
    "                    gold = gold_key[1:]\n",
    "                    assert id_ == gold_key[0]\n",
    "                    sentence_ = ' '.join(\n",
    "                        tokens[:start] + ['[TGT]'] + tokens[start:end] + ['[TGT]'] + tokens[end:]\n",
    "                    )\n",
    "                    write_to_csv(id_,sentence_,lemma,pos,gold)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read semeval2015.gold.txt \n",
    "with open(\"./senseval3.gold.key.txt\") as f:\n",
    "    lines = f.readlines()\n",
    "    lines = [line.strip() for line in lines]\n",
    "\n",
    "# read semeval2015.test.txt\n",
    "with open(\"./senseval3_predictions.txt\") as f:\n",
    "    test_lines = f.readlines()\n",
    "    test_lines = [line.strip() for line in test_lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>sentence</th>\n",
       "      <th>sense_keys</th>\n",
       "      <th>glosses</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d000.s000.t000</td>\n",
       "      <td>How [TGT] long [TGT] has it been since you rev...</td>\n",
       "      <td>['long%3:00:04::', 'long%3:00:02::', 'long%3:0...</td>\n",
       "      <td>['(of speech sounds or syllables) of relativel...</td>\n",
       "      <td>[0,1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d000.s000.t001</td>\n",
       "      <td>How long has it [TGT] been [TGT] since you rev...</td>\n",
       "      <td>['be%2:42:06::', 'be%2:42:02::', 'be%2:42:13::...</td>\n",
       "      <td>['be identical to; be someone or something', '...</td>\n",
       "      <td>[3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d000.s00g0.t002</td>\n",
       "      <td>How long has it been since you [TGT] reviewed ...</td>\n",
       "      <td>['review%2:31:04::', 'review%2:32:00::', 'revi...</td>\n",
       "      <td>[\"refresh one's memory\", 'appraise critically'...</td>\n",
       "      <td>[2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d000.s000.t003</td>\n",
       "      <td>How long has it been since you reviewed the [T...</td>\n",
       "      <td>['objective%1:06:00::', 'objective%1:09:00::']</td>\n",
       "      <td>['the lens or system of lenses in a telescope ...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d000.s000.t004</td>\n",
       "      <td>How long has it been since you reviewed the ob...</td>\n",
       "      <td>['benefit%1:21:00::', 'benefit%1:10:00::', 'be...</td>\n",
       "      <td>['financial assistance in time of need', 'a pe...</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               uid                                           sentence  \\\n",
       "0   d000.s000.t000  How [TGT] long [TGT] has it been since you rev...   \n",
       "1   d000.s000.t001  How long has it [TGT] been [TGT] since you rev...   \n",
       "2  d000.s00g0.t002  How long has it been since you [TGT] reviewed ...   \n",
       "3   d000.s000.t003  How long has it been since you reviewed the [T...   \n",
       "4   d000.s000.t004  How long has it been since you reviewed the ob...   \n",
       "\n",
       "                                          sense_keys  \\\n",
       "0  ['long%3:00:04::', 'long%3:00:02::', 'long%3:0...   \n",
       "1  ['be%2:42:06::', 'be%2:42:02::', 'be%2:42:13::...   \n",
       "2  ['review%2:31:04::', 'review%2:32:00::', 'revi...   \n",
       "3     ['objective%1:06:00::', 'objective%1:09:00::']   \n",
       "4  ['benefit%1:21:00::', 'benefit%1:10:00::', 'be...   \n",
       "\n",
       "                                             glosses target  \n",
       "0  ['(of speech sounds or syllables) of relativel...  [0,1]  \n",
       "1  ['be identical to; be someone or something', '...    [3]  \n",
       "2  [\"refresh one's memory\", 'appraise critically'...    [2]  \n",
       "3  ['the lens or system of lenses in a telescope ...    [1]  \n",
       "4  ['financial assistance in time of need', 'a pe...    [0]  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "data = pd.read_csv('../NN/SemCor/semcor_copy.csv')\n",
    "\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7005405405405405\n"
     ]
    }
   ],
   "source": [
    "\n",
    "count = 0\n",
    "for i in range(len(lines)):\n",
    "    # split the line to get the id and the gloss\n",
    "    line = lines[i].split()\n",
    "    # print(line)\n",
    "    test_line = test_lines[i].split()\n",
    "    for j in range(len(line[1:])):\n",
    "        if test_line[1] == line[j+1]:\n",
    "            count += 1\n",
    "            break\n",
    "\n",
    "print(\"Accuracy:\",count/len(lines))\n",
    "# print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7354198262787812\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(len(lines)):\n",
    "    # split the line to get the id and the gloss\n",
    "    line = lines[i].split()\n",
    "    # print(line)\n",
    "    test_line = test_lines[i].split()\n",
    "    for j in range(len(line[1:])):\n",
    "        if test_line[1] == line[j+1]:\n",
    "            count += 1\n",
    "            break\n",
    "\n",
    "print(\"Accuracy:\", count/len(lines))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchsummary\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n",
      "Installing collected packages: torchsummary\n",
      "Successfully installed torchsummary-1.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BERT_for_WSD: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BERT_for_WSD from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BERT_for_WSD from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BERT_for_WSD were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['ranking_linear.weight', 'ranking_linear.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "BERT_for_WSD.forward() takes 2 positional arguments but 5 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/abhishek/Desktop/Word-Sense-Disambiguation/NN/code.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 60>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/abhishek/Desktop/Word-Sense-Disambiguation/NN/code.ipynb#X12sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m model \u001b[39m=\u001b[39m BERT_for_WSD\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mbert-base-uncased\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/abhishek/Desktop/Word-Sense-Disambiguation/NN/code.ipynb#X12sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m BertTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mbert-base-uncased\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/abhishek/Desktop/Word-Sense-Disambiguation/NN/code.ipynb#X12sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m summary(model, input_size\u001b[39m=\u001b[39;49m[(\u001b[39m32\u001b[39;49m, \u001b[39m128\u001b[39;49m), (\u001b[39m32\u001b[39;49m, \u001b[39m128\u001b[39;49m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/abhishek/Desktop/Word-Sense-Disambiguation/NN/code.ipynb#X12sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m         (\u001b[39m32\u001b[39;49m, \u001b[39m128\u001b[39;49m), (\u001b[39m32\u001b[39;49m,)], device\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/abhishek/Desktop/Word-Sense-Disambiguation/NN/code.ipynb#X12sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m \u001b[39m# model.to(device)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/abhishek/Desktop/Word-Sense-Disambiguation/NN/code.ipynb#X12sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m \u001b[39m# model.eval()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/abhishek/Desktop/Word-Sense-Disambiguation/NN/code.ipynb#X12sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/abhishek/Desktop/Word-Sense-Disambiguation/NN/code.ipynb#X12sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m \u001b[39m#             headers=[\"No.\", \"Sense key\", \"Definition\", \"Score\"])\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/abhishek/Desktop/Word-Sense-Disambiguation/NN/code.ipynb#X12sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m \u001b[39m#         )\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torchsummary/torchsummary.py:72\u001b[0m, in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     68\u001b[0m model\u001b[39m.\u001b[39mapply(register_hook)\n\u001b[1;32m     70\u001b[0m \u001b[39m# make a forward pass\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39m# print(x.shape)\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m model(\u001b[39m*\u001b[39;49mx)\n\u001b[1;32m     74\u001b[0m \u001b[39m# remove these hooks\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[39mfor\u001b[39;00m h \u001b[39min\u001b[39;00m hooks:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: BERT_for_WSD.forward() takes 2 positional arguments but 5 were given"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "import argparse\n",
    "import re\n",
    "\n",
    "import torch\n",
    "from tabulate import tabulate\n",
    "from torch.nn.functional import softmax\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer\n",
    "from createFeatures import GlossSelectionRecord, _create_features_from_records\n",
    "from modelBERT import BERT_for_WSD\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "MAX_SEQ_LENGTH = 128\n",
    "device  = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "def get_predictions(model, tokenizer, sentence):\n",
    "    re_result = re.search(r\"\\[TGT\\](.*)\\[TGT\\]\", sentence)\n",
    "    if re_result is None:\n",
    "        print(\"Incorrect input format\")\n",
    "        return\n",
    "\n",
    "    ambiguous_word = re_result.group(1).strip()\n",
    "    sense_keys = []\n",
    "    definitions = []\n",
    "    for sense_key, definition in get_glosses(None,ambiguous_word).items():\n",
    "        sense_keys.append(sense_key)\n",
    "        definitions.append(definition)\n",
    "\n",
    "    record = GlossSelectionRecord(\n",
    "        \"test\", sentence, sense_keys, definitions, [-1])\n",
    "    features = _create_features_from_records([record], MAX_SEQ_LENGTH, tokenizer,\n",
    "                                             cls_token=tokenizer.cls_token,\n",
    "                                             sep_token=tokenizer.sep_token,\n",
    "                                             cls_token_segment_id=1,\n",
    "                                             pad_token_segment_id=0,\n",
    "                                             disable_progress_bar=True)[0]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = torch.zeros(len(definitions), dtype=torch.double).to(device)\n",
    "        for i, bert_input in tqdm(list(enumerate(features)), desc=\"Progress\"):\n",
    "            logits[i] = model.ranking_linear(\n",
    "                model.bert(\n",
    "                    input_ids=torch.tensor(\n",
    "                        bert_input.input_ids, dtype=torch.long).unsqueeze(0).to(device),\n",
    "                    attention_mask=torch.tensor(\n",
    "                        bert_input.input_mask, dtype=torch.long).unsqueeze(0).to(device),\n",
    "                    token_type_ids=torch.tensor(\n",
    "                        bert_input.segment_ids, dtype=torch.long).unsqueeze(0).to(device)\n",
    "                )[1]\n",
    "            )\n",
    "        scores = softmax(logits, dim=0)\n",
    "\n",
    "    return sorted(zip(sense_keys, definitions, scores), key=lambda x: x[-1], reverse=True)\n",
    "\n",
    "\n",
    "model = BERT_for_WSD.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "summary(model, input_size=[(32, 128), (32, 128),\n",
    "        (32, 128), (32,)], device='cuda')\n",
    "# model.to(device)\n",
    "# model.eval()\n",
    "\n",
    "# while True:\n",
    "#     sentence = input(\n",
    "#         \"\\nEnter a sentence with an ambiguous word surrounded by [TGT] tokens\\n> \")\n",
    "#     predictions = get_predictions(model, tokenizer, sentence)\n",
    "#     if predictions:\n",
    "#         print(\"\\nPredictions:\")\n",
    "#         print(tabulate(\n",
    "#             [[f\"{i+1}.\", key, gloss, f\"{score:.5f}\"]\n",
    "#              for i, (key, gloss, score) in enumerate(predictions)],\n",
    "#             headers=[\"No.\", \"Sense key\", \"Definition\", \"Score\"])\n",
    "#         )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
