{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import semcor\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from transformers import BertTokenizer\n",
    "import pandas as pd \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./SemCor/semcor_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('./SemCor/all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def getNewData(data):\n",
    "    new_data = pd.DataFrame(columns=['sentence','target_word', 'sense', 'gloss'])\n",
    "\n",
    "    for i in tqdm(range(0,len(data))):\n",
    "        sentence = data.iloc[i]['sentence']\n",
    "        idx1 = sentence.find('[TGT]')\n",
    "        idx2 = sentence.find('[TGT]', idx1+1)\n",
    "        target_word = sentence[idx1+6:idx2-1]\n",
    "        sentence = sentence.replace('[TGT]', '')\n",
    "        sense_keys = data.iloc[i]['sense_keys']\n",
    "        glosses = data.iloc[i]['glosses']\n",
    "        target = data.iloc[i]['target']\n",
    "        sense_keys = sense_keys.strip('[]')\n",
    "        sense_keys = sense_keys.split(',')\n",
    "        target = target.strip('[]')\n",
    "        target = target.split(',')\n",
    "        glosses = glosses.strip('[]')\n",
    "        glosses = glosses.split(',')\n",
    "        # for every target value add the correspodinign sense key in a new column and also a new column for the gloss\n",
    "        for j in range(0,len(target)):\n",
    "            tgt = int(target[j])\n",
    "            new_row = {'sentence': sentence, 'sense': sense_keys[tgt], 'gloss': glosses[tgt], 'target_word': target_word}\n",
    "            new_data = pd.concat([new_data, pd.DataFrame(new_row, index=[0])], ignore_index=True)\n",
    "            new_data['sense'] = new_data['sense'].str.replace('\"', '')\n",
    "            new_data['sense'] = new_data['sense'].str.replace(\"'\", '')\n",
    "    return new_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [07:31<00:00, 66.47it/s] \n"
     ]
    }
   ],
   "source": [
    "train_data = getNewData(train_data[:30000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7253/7253 [00:40<00:00, 180.46it/s]\n"
     ]
    }
   ],
   "source": [
    "test_data = getNewData(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>target_word</th>\n",
       "      <th>sense</th>\n",
       "      <th>gloss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How  long  has it been since you reviewed the ...</td>\n",
       "      <td>long</td>\n",
       "      <td>long%3:00:02::</td>\n",
       "      <td>'primarily temporal sense; being or indicatin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How long has it  been  since you reviewed the ...</td>\n",
       "      <td>been</td>\n",
       "      <td>be%2:42:03::</td>\n",
       "      <td>undisturbed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How long has it been since you  reviewed  the ...</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>review%2:31:00::</td>\n",
       "      <td>'look at again; examine again'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How long has it been since you reviewed the  o...</td>\n",
       "      <td>objectives</td>\n",
       "      <td>objective%1:09:00::</td>\n",
       "      <td>'the goal intended to be attained (and which ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How long has it been since you reviewed the ob...</td>\n",
       "      <td>benefit</td>\n",
       "      <td>benefit%1:21:00::</td>\n",
       "      <td>'financial assistance in time of need'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>How long has it been since you reviewed the ob...</td>\n",
       "      <td>service</td>\n",
       "      <td>service%1:04:07::</td>\n",
       "      <td>'employment in or work for another'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How long has it been since you reviewed the ob...</td>\n",
       "      <td>program</td>\n",
       "      <td>program%1:09:01::</td>\n",
       "      <td>'a system of projects or services intended to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Have you  permitted  it to become a giveaway p...</td>\n",
       "      <td>permitted</td>\n",
       "      <td>permit%2:41:00::</td>\n",
       "      <td>'make it possible through a specific action o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Have you permitted it to  become  a giveaway p...</td>\n",
       "      <td>become</td>\n",
       "      <td>become%2:42:01::</td>\n",
       "      <td>'undergo a change or development'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Have you permitted it to become a  giveaway  p...</td>\n",
       "      <td>giveaway</td>\n",
       "      <td>giveaway%1:21:00::</td>\n",
       "      <td>'a gift of public land or resources for the pr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence target_word  \\\n",
       "0  How  long  has it been since you reviewed the ...        long   \n",
       "1  How long has it  been  since you reviewed the ...        been   \n",
       "2  How long has it been since you  reviewed  the ...    reviewed   \n",
       "3  How long has it been since you reviewed the  o...  objectives   \n",
       "4  How long has it been since you reviewed the ob...     benefit   \n",
       "5  How long has it been since you reviewed the ob...     service   \n",
       "6  How long has it been since you reviewed the ob...     program   \n",
       "7  Have you  permitted  it to become a giveaway p...   permitted   \n",
       "8  Have you permitted it to  become  a giveaway p...      become   \n",
       "9  Have you permitted it to become a  giveaway  p...    giveaway   \n",
       "\n",
       "                  sense                                              gloss  \n",
       "0        long%3:00:02::   'primarily temporal sense; being or indicatin...  \n",
       "1          be%2:42:03::                                        undisturbed  \n",
       "2      review%2:31:00::                     'look at again; examine again'  \n",
       "3   objective%1:09:00::   'the goal intended to be attained (and which ...  \n",
       "4     benefit%1:21:00::             'financial assistance in time of need'  \n",
       "5     service%1:04:07::                'employment in or work for another'  \n",
       "6     program%1:09:01::   'a system of projects or services intended to...  \n",
       "7      permit%2:41:00::   'make it possible through a specific action o...  \n",
       "8      become%2:42:01::                  'undergo a change or development'  \n",
       "9    giveaway%1:21:00::  'a gift of public land or resources for the pr...  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_word_idx = {}\n",
    "idx_to_target = {}\n",
    "sense_labels = []\n",
    "lemma_2_sense = {}\n",
    "for i in range(0,len(train_data)):\n",
    "    sense_label = train_data.iloc[i]['sense']\n",
    "    sense_label = sense_label.replace(' ','')\n",
    "    lemma, pos, wnsn,wnsn2 = sense_label.split('%')[0], int(sense_label.split(\n",
    "        '%')[1].split(':')[0]), sense_label.split('%')[1].split(':')[1],sense_label.split('%')[1].split(':')[2]\n",
    "    new_label = lemma + '%' + str(pos) + '%' + wnsn + '%' + wnsn2\n",
    "    if lemma not in lemma_2_sense:\n",
    "        lemma_2_sense[lemma] = []\n",
    "        target_word_idx[lemma] = len(target_word_idx)\n",
    "        idx_to_target[len(idx_to_target)] = lemma\n",
    "    if sense_label not in lemma_2_sense[lemma]:\n",
    "        lemma_2_sense[lemma].append(sense_label)\n",
    "    # sense_labels.append(new_label)\n",
    "target_word_idx['<unk>'] = len(target_word_idx)\n",
    "idx_to_target[len(idx_to_target)] = '<unk>'\n",
    "lemma_2_sense['<unk>'] = ['<unk>']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['long%3:00:02::',\n",
       " 'long%3:00:01::',\n",
       " 'long%4:02:00::',\n",
       " 'long%2:37:02::',\n",
       " 'long%5:00:00:tall:00']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_2_sense['long']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_data['sense_label'] = sense_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save \n",
    "new_data.to_csv('./SemCor/semcor_lstm.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pd.read_csv('./SemCor/semcor_lstm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the max length of the sentence \n",
    "\n",
    "class preProcessDataset():\n",
    "    def __init__(self,data,min_freq):\n",
    "        self.data = data\n",
    "        self.min_freq = min_freq\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.vocab = []\n",
    "        self.vocab_sense = []\n",
    "        self.sense2idx = {}\n",
    "        self.idx2sense = {}\n",
    "        self.max_len = 0\n",
    "        self.wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "        self.lemma_2_sense = {}\n",
    "        self.wordFreq = {}\n",
    "        self.target2idx = target_word_idx\n",
    "        self.word2idx['<pad>'] = len(self.word2idx)\n",
    "        self.idx2word[len(self.idx2word)] = '<pad>'\n",
    "        self.vocab.append('<pad>')\n",
    "        self.vocab.append('<unk>')\n",
    "        self.word2idx['<unk>'] = len(self.word2idx)\n",
    "        self.idx2word[len(self.idx2word)] = '<unk>'\n",
    "        self.vocab_sense.append('<unk>')\n",
    "        self.sense2idx['<unk>'] = len(self.sense2idx)\n",
    "        self.idx2sense[len(self.idx2sense)] = '<unk>'\n",
    "        \n",
    "\n",
    "\n",
    "        data = self.data\n",
    "        for i in tqdm(range(len(data))):\n",
    "            sentence = data.iloc[i]['sentence']\n",
    "            target_word = data.iloc[i]['sense'].split('%')[0]\n",
    "            target_word = target_word.lower()\n",
    "            target_word = target_word.replace(' ','')\n",
    "            sense_keys = data.iloc[i]['sense']\n",
    "            sense_keys = sense_keys.replace(' ','')\n",
    "\n",
    "            sentence = sentence.split()\n",
    "            # count freq of words\n",
    "            for word in sentence:\n",
    "                word = word.lower()\n",
    "                if word not in self.wordFreq:\n",
    "                    self.wordFreq[word] = 0\n",
    "                self.wordFreq[word] += 1\n",
    "\n",
    "\n",
    "            for word in sentence:\n",
    "                word = word.lower()\n",
    "                # punctuation marks\n",
    "                if word in ['.',',','?','!',';',':','(',')','[',']','{','}',\"'\",'\"']:\n",
    "                    continue\n",
    "                if self.wordFreq[word] < self.min_freq:\n",
    "                    word = '<unk>'\n",
    "                if word not in self.word2idx:\n",
    "                    self.word2idx[word] = len(self.word2idx)\n",
    "                    self.idx2word[len(self.idx2word)] = word\n",
    "                    self.vocab.append(word)\n",
    "            if len(sentence) > self.max_len:\n",
    "                self.max_len = len(sentence)\n",
    "            if sense_keys not in self.sense2idx:\n",
    "                self.sense2idx[sense_keys] = len(self.sense2idx)\n",
    "                self.idx2sense[len(self.idx2sense)] = sense_keys\n",
    "                self.vocab_sense.append(sense_keys)\n",
    "                \n",
    "\n",
    "class getDataset(Dataset):\n",
    "    def __init__(self, data, word2idx, sense2idx, max_len, target2idx,idx2word,wordFreq,vocab):\n",
    "        self.data = data\n",
    "        self.word2idx = word2idx\n",
    "        self.sense2idx = sense2idx\n",
    "        self.max_len = max_len\n",
    "        self.target2idx = target2idx\n",
    "        self.idx2word = idx2word\n",
    "        self.wordFreq = wordFreq\n",
    "        self.vocab = vocab\n",
    "        self.input_data = []\n",
    "        self.sense_data = []\n",
    "        self.target2word = []\n",
    "\n",
    "\n",
    "        for i in tqdm(range(len(data))):\n",
    "            sentence = data.iloc[i]['sentence']\n",
    "            sense_keys = data.iloc[i]['sense']\n",
    "            sense_keys = sense_keys.replace(' ','')\n",
    "            target_word = sense_keys.split('%')[0]\n",
    "            target_word = target_word.lower()\n",
    "\n",
    "            target_word = target_word.replace(' ','')\n",
    "            sense_keys = sense_keys.replace(' ','')\n",
    "            sentence = sentence.split()\n",
    "            sentence_idx = []\n",
    "            sense_idx = []\n",
    "            for word in sentence:\n",
    "                word = word.lower()\n",
    "                # punctuation marks\n",
    "                if word in ['.',',','?','!',';',':','(',')','[',']','{','}',\"'\",'\"']:\n",
    "                    continue\n",
    "                if word not in self.word2idx:\n",
    "                    word = '<unk>'\n",
    "                sentence_idx.append(self.word2idx[word])\n",
    "            while len(sentence_idx) < self.max_len:\n",
    "                sentence_idx.append(self.word2idx['<pad>'])\n",
    "            self.input_data.append(sentence_idx)\n",
    "            # sense_idx.append(self.sense2idx[sense_keys])\n",
    "            if sense_keys not in self.sense2idx:\n",
    "                sense_keys = '<unk>'\n",
    "            self.sense_data.append(self.sense2idx[sense_keys])\n",
    "            if target_word not in self.target2idx:\n",
    "                target_word = '<unk>'\n",
    "            self.target2word.append(self.target2idx[target_word])\n",
    "        \n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return torch.tensor(self.input_data[idx]),torch.tensor(self.sense_data[idx]),torch.tensor(self.target2word[idx])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30258/30258 [00:05<00:00, 5240.73it/s]\n"
     ]
    }
   ],
   "source": [
    "preProcessDataset = preProcessDataset(train_data,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30258/30258 [00:07<00:00, 4173.79it/s]\n",
      "100%|██████████| 1888/1888 [00:00<00:00, 3657.08it/s]\n"
     ]
    }
   ],
   "source": [
    "trainData = getDataset(train_data,preProcessDataset.word2idx,preProcessDataset.sense2idx,preProcessDataset.max_len,preProcessDataset.target2idx,preProcessDataset.idx2word,preProcessDataset.wordFreq,preProcessDataset.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7611/7611 [00:01<00:00, 4041.90it/s]\n"
     ]
    }
   ],
   "source": [
    "testData = getDataset(test_data, preProcessDataset.word2idx, preProcessDataset.sense2idx, preProcessDataset.max_len,\n",
    "                      preProcessDataset.target2idx, preProcessDataset.idx2word, preProcessDataset.wordFreq, preProcessDataset.vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_dataset, valid_dataset = train_test_split(trainData, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=16, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(testData, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class biLSTMModel(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,sense_vocab,embedding_dim,dataset):\n",
    "        super(biLSTMModel,self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.sense_vocab = sense_vocab\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.dataset = dataset\n",
    "        self.embedding = nn.Embedding(self.input_size,self.embedding_dim)\n",
    "        self.lstm = nn.LSTM(self.embedding_dim,self.hidden_size,bidirectional=True)\n",
    "        self.linear = nn.Linear(self.hidden_size*2,len(self.sense_vocab))\n",
    "        self.sense2idx = self.dataset.sense2idx\n",
    "        self.idx2word = idx_to_target\n",
    "\n",
    "    def forward(self,x,target_word):\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(1,0,2)\n",
    "        output,(hidden,cell) = self.lstm(x)\n",
    "        hidden = torch.cat((hidden[-2,:,:],hidden[-1,:,:]),dim=1)\n",
    "        out = self.linear(hidden)\n",
    "        for i,target_wo in enumerate(target_word):\n",
    "            target_wo = idx_to_target[target_wo.item()]\n",
    "            target_word_sense = lemma_2_sense[target_wo]\n",
    "            target_word_sense_idx = [self.sense2idx[sense] for sense in target_word_sense]\n",
    "            out[i,target_word_sense_idx] = F.softmax(out[i,target_word_sense_idx],dim=0)\n",
    "\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biLSTMModel(\n",
      "  (embedding): Embedding(9283, 300)\n",
      "  (lstm): LSTM(300, 128, bidirectional=True)\n",
      "  (linear): Linear(in_features=256, out_features=10356, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torch.nn import CrossEntropyLoss\n",
    "model = biLSTMModel(len(preProcessDataset.word2idx),128,preProcessDataset.vocab_sense,300,preProcessDataset)\n",
    "model = model.cuda()\n",
    "criterion = CrossEntropyLoss(ignore_index=preProcessDataset.word2idx['<pad>'])\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "634\n"
     ]
    }
   ],
   "source": [
    "# print(len(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1/10 | Loss : 0.8970 | Accuracy : 0.6899\n",
      "Epoch : 1/10 | Validation Loss : 0.7616 | Validation Accuracy : 0.7095\n",
      "Epoch : 2/10 | Loss : 0.7125 | Accuracy : 0.7450\n",
      "Epoch : 2/10 | Validation Loss : 0.7578 | Validation Accuracy : 0.7108\n",
      "Epoch : 3/10 | Loss : 0.7048 | Accuracy : 0.7512\n",
      "Epoch : 3/10 | Validation Loss : 0.7538 | Validation Accuracy : 0.7161\n",
      "Epoch : 4/10 | Loss : 0.7021 | Accuracy : 0.7534\n",
      "Epoch : 4/10 | Validation Loss : 0.7518 | Validation Accuracy : 0.7160\n",
      "Epoch : 5/10 | Loss : 0.7028 | Accuracy : 0.7527\n",
      "Epoch : 5/10 | Validation Loss : 0.7523 | Validation Accuracy : 0.7150\n",
      "Epoch : 6/10 | Loss : 0.7012 | Accuracy : 0.7541\n",
      "Epoch : 6/10 | Validation Loss : 0.7522 | Validation Accuracy : 0.7156\n",
      "Epoch : 7/10 | Loss : 0.7003 | Accuracy : 0.7546\n",
      "Epoch : 7/10 | Validation Loss : 0.7550 | Validation Accuracy : 0.7143\n",
      "Epoch : 8/10 | Loss : 0.6991 | Accuracy : 0.7561\n",
      "Epoch : 8/10 | Validation Loss : 0.7530 | Validation Accuracy : 0.7135\n",
      "Epoch : 9/10 | Loss : 0.6992 | Accuracy : 0.7558\n",
      "Epoch : 9/10 | Validation Loss : 0.7548 | Validation Accuracy : 0.7120\n",
      "Epoch : 10/10 | Loss : 0.6984 | Accuracy : 0.7566\n",
      "Epoch : 10/10 | Validation Loss : 0.7549 | Validation Accuracy : 0.7125\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    total_correct = 0\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for i,(sentence,sense,target_word) in enumerate(train_dataloader):\n",
    "        sentence = sentence.cuda()\n",
    "        sense = sense.cuda()\n",
    "        target_word = target_word.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(sentence,target_word)\n",
    "        # print(output)\n",
    "        loss = criterion(output,sense)\n",
    "        pred_sense = torch.argmax(output,dim=1)\n",
    "        correct = torch.sum(pred_sense == sense)\n",
    "        total_correct += correct.item()\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    print('Epoch : {}/{} | Loss : {:.4f} | Accuracy : {:.4f}'.format(epoch+1,num_epochs,total_loss/len(train_dataloader),total_correct/len(train_dataset)))\n",
    "\n",
    "    # validation on test dataset \n",
    "    total_correct = 0\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i,(sentence,sense,target_word) in enumerate(valid_dataloader):\n",
    "            sentence = sentence.cuda()\n",
    "            sense = sense.cuda()\n",
    "            target_word = target_word.cuda()\n",
    "            output = model(sentence,target_word)\n",
    "            loss = criterion(output,sense)\n",
    "            pred_sense = torch.argmax(output,dim=1)\n",
    "            correct = torch.sum(pred_sense == sense)\n",
    "            total_correct += correct.item()\n",
    "            total_loss += loss.item()\n",
    "        print('Epoch : {}/{} | Validation Loss : {:.4f} | Validation Accuracy : {:.4f}'.format(epoch+1,num_epochs,total_loss/len(valid_dataloader),total_correct/len(valid_dataset)))\n",
    "\n",
    "torch.save(model.state_dict(),'./SemCor/biLSTM_model.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Testing Loss : 0.8730 | Testing Accuracy : 0.6182\n"
     ]
    }
   ],
   "source": [
    "total_correct = 0\n",
    "total_loss = 0\n",
    "with torch.no_grad():\n",
    "    for i, (sentence, sense, target_word) in enumerate(test_dataloader):\n",
    "        sentence = sentence.cuda()\n",
    "        sense = sense.cuda()\n",
    "        target_word = target_word.cuda()\n",
    "        output = model(sentence, target_word)\n",
    "        loss = criterion(output, sense)\n",
    "        pred_sense = torch.argmax(output, dim=1)\n",
    "        correct = torch.sum(pred_sense == sense)\n",
    "        total_correct += correct.item()\n",
    "        total_loss += loss.item()\n",
    "    print('| Testing Loss : {:.4f} | Testing Accuracy : {:.4f}'.format(\n",
    "        total_loss/len(test_dataloader), total_correct/len(testData)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
